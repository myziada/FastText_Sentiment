{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0t3p3gbYaQG",
        "outputId": "eb203415-34d8-4abc-e381-e7911bae004e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7_8yB6_Ycdx",
        "outputId": "6e80fef5-bf68-4c8f-f801-0b16fa2ee105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH-ykqXy0OIr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACd6SrlXmyA8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "\n",
        "#import the required library\n",
        "\n",
        "# Student will have to code here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,MaxPooling1D\n",
        "from keras.layers import LSTM, Flatten, Dropout, Conv1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0VZ5uGUoWdi",
        "outputId": "45a9f8dc-75b0-4639-dbce-7fb4d058ad12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WmSk7tnmySV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.load.__defaults__=(None, True, True, 'ASCII')\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7qM93KXoqaJ",
        "outputId": "2dd5f93f-06ab-4f21-9aed-a11d9ee7734a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (20000,)\n",
            "Shape of Test data: (25000,)\n",
            "Shape of CV data: (5000,)\n"
          ]
        }
      ],
      "source": [
        "X_train,X_cv,y_train,y_cv = train_test_split(X_train,y_train,test_size = 0.2)\n",
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)\n",
        "print(\"Shape of CV data:\", X_cv.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAK9xPpaoqdp"
      },
      "outputs": [],
      "source": [
        "max_review_length = 600\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "X_cv = sequence.pad_sequences(X_cv, maxlen=max_review_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CgUrD9Q1xwt",
        "outputId": "45ca9aa7-b124-4ad7-a646-ac4aee9884a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "y_train[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcM-QWwwmy4P",
        "outputId": "a6601a9e-aa8f-4f40-e232-b6a2fd960948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # i had intended to # the # # of # # passing with numerous # films of his that i own on vhs however given my ongoing light hearted christmas marathon i had to make do with just this one as it happens it features one of his best performances and he was # oscar nominated for it with the film itself being likewise # this was also one of 14 # with that other most widely recognized star to emerge from italy sophia loren both incidentally are playing against type here she as an # housewife and he a homosexual br br by the way the film's title has a double meaning the leading characters are brought together on the historic day in which hitler came to italy to meet # the event itself being shown in lengthy archive footage but it more specifically refers to the # # # in which they share moments of friendship revelation and briefly passion though each knows that a return to their normal existence is inevitable which leads to the film's abrupt bittersweet ending this is virtually a two # with all other characters save for the # # of the apartment block in which the story takes place in its entirety which include # gruff and # patriotic husband surprisingly played by john # appear only at the beginning and closing sequences still the # setting doesn't # director # for the record this is the # film of his that i've watched and own 3 more on vhs and cinematographer # de # so that the result though essentially low key is far from # the camera is allowed to # the various sections of the large building # the proceedings # or # as the situation requires but always # br br the narrative of course depends entirely on the performances of the two stars for it to be convincing and they both deliver their on screen chemistry is quite # it's interesting however that while loren walked away with the # in their home # it's # moving yet # # the film somewhat # does seem to # his sexual # with anti # who generally impressed international audiences\n"
          ]
        }
      ],
      "source": [
        "# Decoding the data coded data of IMDB ( Data Understanding )\n",
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in X_train[0]] )\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtdup-novHAJ"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8WMnDjvvHsp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, concatenate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiRTf5PPd1bP",
        "outputId": "6ddc5f29-8109-40af-b066-47af55f4e358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393236 sha256=32390aa30c5053472b4f1ca9a24937bb729f7b367d9e5fbcc6120f437930967c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.FastText.eprint = lambda x: None"
      ],
      "metadata": {
        "id": "botqnWsxBJ1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Baca dataset tweet ke dalam DataFrame\n",
        "df = pd.read_csv('Training_Testing_Data16444.csv')\n",
        "\n",
        "# Preprocessing teks jika diperlukan\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Gabungkan label dan teks ke dalam satu kolom dengan format yang diperlukan\n",
        "df['fasttext_input'] = '__label__' + df['Label'].astype(str) + ' ' + df['text']\n",
        "\n",
        "# Split data menjadi training dan test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.20, random_state=42)\n",
        "\n",
        "# Simpan training dan test sets ke file terpisah\n",
        "train_data['fasttext_input'].to_csv('cooking.train', index=False, header=False, sep='\\t')\n",
        "test_data['fasttext_input'].to_csv('cooking.test', index=False, header=False, sep='\\t')\n",
        "\n",
        "# Train model FastText\n",
        "model = fasttext.train_supervised(input=\"cooking.train\", lr=0.1, epoch=20, wordNgrams=5, bucket=200000, dim=100, loss='hs')\n",
        "\n",
        "# Simpan model yang dilatih\n",
        "model.save_model(\"tweet_model.bin\")\n",
        "\n",
        "# Load model yang dilatih\n",
        "model = fasttext.load_model('tweet_model.bin')\n",
        "\n",
        "# Evaluasi model pada data test\n",
        "result = model.test('cooking.test')\n",
        "\n",
        "# Ekstrak metrik evaluasi dari tuple hasil\n",
        "precision = result[1]\n",
        "recall = result[2]\n",
        "num_examples = result[0]\n",
        "\n",
        "# Cetak hasil evaluasi\n",
        "print('Akurasi:', precision)\n",
        "print('Recall:', recall)\n",
        "print('Jumlah contoh:', num_examples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLr4aJfSdvxD",
        "outputId": "b861005d-a458-4cb2-d930-94839acf39cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.8275477626311255\n",
            "Recall: 0.8275477626311255\n",
            "Jumlah contoh: 19733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "with open('cooking.test', 'r', encoding='utf-8') as file:\n",
        "    test_dataset = file.readlines()\n",
        "# Calculate accuracy manually\n",
        "total_examples = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "for line in test_dataset:\n",
        "    label, text = line.strip().split(' ', 1)\n",
        "    prediction = model.predict(text)[0][0]\n",
        "    if prediction == label:\n",
        "        correct_predictions += 1\n",
        "    total_examples += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MsvePWtF4gJ",
        "outputId": "45b26851-cfdf-427c-c855-84d7e495efba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7849266196383686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Load the trained model\n",
        "# model = fasttext.load_model('tweet_model.bin')\n",
        "\n",
        "# # Load the test data\n",
        "# test_data = pd.read_csv('cooking.test', header=None, sep='\\t')[0].tolist()\n",
        "# test_labels = pd.read_csv('cooking.test', header=None, sep='\\t', usecols=[0], names=['Label'])['Label'].apply(lambda x: int(x.replace('__label__', ''))).tolist()\n",
        "\n",
        "# # Predict labels for test data\n",
        "# predicted_labels = [int(label[0].replace('__label__', '')) for label in model.predict(test_data)[0]]\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "\n",
        "# # Print the accuracy\n",
        "# print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "kfRJOgvqBOUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "\n",
        "#import the required library\n",
        "import numpy as np\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,MaxPooling1D\n",
        "from keras.layers import LSTM, Flatten, Dropout, Conv1D, Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, LSTM, concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import regularizers\n",
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "fkW0z2qtZ5wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'text' is the column containing the text data in your DataFrame\n",
        "texts = df['text'].tolist()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Limit the tokenizer vocabulary to the top n words\n",
        "top_n = 10000  # Set the desired number of top words\n",
        "tokenizer.num_words = top_n\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n"
      ],
      "metadata": {
        "id": "tGVPnmMBaA6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung panjang maksimum dari sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "print(\"Maximum sequence length:\", max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0mRk5_SZ0Of",
        "outputId": "10c15e66-e100-4998-ef42-c9b9666bad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'label' is the column containing the corresponding labels in your DataFrame\n",
        "labels = df['Label'].tolist()\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = max(len(s) for s in sequences)  # Set the maximum sequence length based on the length of the longest sequence\n",
        "# Set the desired maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the training data further into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the vocabulary size\n",
        "vocab_size = min(top_n, len(tokenizer.word_index)) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wlov3KkZ0Rf",
        "outputId": "9ea06ed7-c274-4f73-b4a0-e4c5b2844fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a function to encode labels and handle unseen labels\n",
        "def encode_labels(labels, classes):\n",
        "    encoded_labels = np.zeros((len(labels), len(classes)))\n",
        "    for i, label in enumerate(labels):\n",
        "        if label in classes:\n",
        "            encoded_labels[i, classes.index(label)] = 1\n",
        "        else:\n",
        "            encoded_labels[i, 0] = 1  # Assign unseen labels to the first class\n",
        "    return encoded_labels\n",
        "\n",
        "# Define the classes based on the unique labels in the original label list\n",
        "classes = np.unique(labels).tolist()\n",
        "\n",
        "# Encode labels for training, validation, and test sets\n",
        "y_train = encode_labels(y_train, classes)\n",
        "y_val = encode_labels(y_val, classes)\n",
        "y_test = encode_labels(y_test, classes)\n"
      ],
      "metadata": {
        "id": "GX-yBR6VaFao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)\n",
        "print(\"Shape of CV data:\", X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQU9gmgXaIfP",
        "outputId": "ab644eb3-d6e0-4fcb-dff2-4e6c1fb45469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (63144, 250)\n",
            "Shape of Test data: (19733, 250)\n",
            "Shape of CV data: (15787, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtUzqejmpqOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6f24bb-c1dd-4459-9fb8-8125e056fb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 250)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 250, 100)     26092300    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 248, 128)     38528       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 247, 128)     51328       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 246, 128)     64128       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 124, 128)    0           ['conv1d[0][0]']                 \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " average_pooling1d_1 (AveragePo  (None, 123, 128)    0           ['conv1d_1[0][0]']               \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " average_pooling1d_2 (AveragePo  (None, 123, 128)    0           ['conv1d_2[0][0]']               \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['average_pooling1d[0][0]']      \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['average_pooling1d_1[0][0]']    \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 128)         0           ['average_pooling1d_2[0][0]']    \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 384)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 , 'global_average_pooling1d_1[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 25000)        0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 384)          0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 25000)        0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           6160        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 64)           42240       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          3200128     ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 16)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64)           0           ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 208)          0           ['dropout_1[0][0]',              \n",
            "                                                                  'dropout_2[0][0]',              \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 208)          0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 6)            1254        ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29,496,066\n",
            "Trainable params: 3,403,766\n",
            "Non-trainable params: 26,092,300\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, AveragePooling1D, GlobalAveragePooling1D, Dropout, LSTM, concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained FastText model\n",
        "fasttext_model = fasttext.load_model('tweet_model.bin')\n",
        "\n",
        "# Get the word vectors\n",
        "word_vectors = fasttext_model.get_input_matrix()\n",
        "\n",
        "# Create the embedding matrix\n",
        "vocab_size = word_vectors.shape[0]\n",
        "embedding_vector_length = word_vectors.shape[1]\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_vector_length))\n",
        "word_index = {word: idx for idx, word in enumerate(fasttext_model.words)}\n",
        "for word, index in word_index.items():\n",
        "    embedding_matrix[index] = word_vectors[index]\n",
        "\n",
        "# Define the model architecture\n",
        "max_sequence_length = 250\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False)(input_layer)\n",
        "\n",
        "# CNN Layers\n",
        "cnn_layer1 = Conv1D(128, kernel_size=3, activation='relu')(embedding_layer)\n",
        "cnn_layer1 = AveragePooling1D(pool_size=2)(cnn_layer1)\n",
        "cnn_layer2 = Conv1D(128, kernel_size=4, activation='relu')(embedding_layer)\n",
        "cnn_layer2 = AveragePooling1D(pool_size=2)(cnn_layer2)\n",
        "cnn_layer3 = Conv1D(128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "cnn_layer3 = AveragePooling1D(pool_size=2)(cnn_layer3)\n",
        "cnn_layer1 = GlobalAveragePooling1D()(cnn_layer1)\n",
        "cnn_layer2 = GlobalAveragePooling1D()(cnn_layer2)\n",
        "cnn_layer3 = GlobalAveragePooling1D()(cnn_layer3)\n",
        "cnn_layer = concatenate([cnn_layer1, cnn_layer2, cnn_layer3])\n",
        "cnn_layer = Dropout(0.5)(cnn_layer)\n",
        "cnn_layer = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01))(cnn_layer)\n",
        "cnn_layer = Dropout(0.5)(cnn_layer)\n",
        "\n",
        "# LSTM Layer\n",
        "lstm_layer = LSTM(64)(embedding_layer)\n",
        "lstm_layer = Dropout(0.5)(lstm_layer)\n",
        "\n",
        "# Embedding Layer\n",
        "embedding_only_layer = Flatten()(embedding_layer)\n",
        "embedding_only_layer = Dropout(0.5)(embedding_only_layer)\n",
        "embedding_only_layer = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding_only_layer)\n",
        "embedding_only_layer = Dropout(0.5)(embedding_only_layer)\n",
        "\n",
        "# Concatenate all layers\n",
        "merged_layer = concatenate([cnn_layer, lstm_layer, embedding_only_layer])\n",
        "merged_layer = Dropout(0.5)(merged_layer)\n",
        "output_layer = Dense(6, activation='softmax')(merged_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the training data further into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the data splits\n",
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of validation data:\", X_val.shape)\n",
        "print(\"Shape of test data:\", X_test.shape)\n",
        "print(\"Shape of train labels:\", len(y_train))\n",
        "print(\"Shape of validation labels:\", len(y_val))\n",
        "print(\"Shape of test labels:\", len(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0OzQOApgQt1",
        "outputId": "6b420e8d-6923-4ef4-ad8b-66ba3a48aadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (63144, 250)\n",
            "Shape of validation data: (15787, 250)\n",
            "Shape of test data: (19733, 250)\n",
            "Shape of train labels: 63144\n",
            "Shape of validation labels: 15787\n",
            "Shape of test labels: 19733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "Wj2Jik1xsoUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode class labels with integers\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_train_encoded = to_categorical(y_train_encoded, num_classes)\n",
        "y_val_encoded = to_categorical(y_val_encoded, num_classes)\n",
        "y_test_encoded = to_categorical(y_test_encoded, num_classes)\n",
        "\n",
        "# Train the model with early stopping\n",
        "batch_size = 1020\n",
        "epochs = 10\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs,\n",
        "                    validation_data=(X_val, y_val_encoded), verbose=1,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on training and test data\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded, verbose=0)\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print(f\"Train Loss: {train_loss:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cChoBVTgUwu",
        "outputId": "74332bab-9834-4f40-d0e0-5e4397a142e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "62/62 [==============================] - 331s 5s/step - loss: 3.1988 - accuracy: 0.2293 - val_loss: 2.3704 - val_accuracy: 0.2550\n",
            "Epoch 2/10\n",
            "62/62 [==============================] - 323s 5s/step - loss: 2.1062 - accuracy: 0.2636 - val_loss: 1.9177 - val_accuracy: 0.2681\n",
            "Epoch 3/10\n",
            "62/62 [==============================] - 329s 5s/step - loss: 1.8624 - accuracy: 0.2773 - val_loss: 1.7903 - val_accuracy: 0.2842\n",
            "Epoch 4/10\n",
            "62/62 [==============================] - 323s 5s/step - loss: 1.7750 - accuracy: 0.2854 - val_loss: 1.7180 - val_accuracy: 0.2914\n",
            "Epoch 5/10\n",
            "62/62 [==============================] - 327s 5s/step - loss: 1.7281 - accuracy: 0.2917 - val_loss: 1.7069 - val_accuracy: 0.2934\n",
            "Epoch 6/10\n",
            "62/62 [==============================] - 323s 5s/step - loss: 1.6990 - accuracy: 0.2965 - val_loss: 1.6667 - val_accuracy: 0.2943\n",
            "Epoch 7/10\n",
            "62/62 [==============================] - 310s 5s/step - loss: 1.6724 - accuracy: 0.2989 - val_loss: 1.6549 - val_accuracy: 0.3040\n",
            "Epoch 8/10\n",
            "62/62 [==============================] - 323s 5s/step - loss: 1.6633 - accuracy: 0.3048 - val_loss: 1.6422 - val_accuracy: 0.3083\n",
            "Epoch 9/10\n",
            "62/62 [==============================] - 322s 5s/step - loss: 1.6560 - accuracy: 0.3070 - val_loss: 1.6373 - val_accuracy: 0.3136\n",
            "Epoch 10/10\n",
            "62/62 [==============================] - 327s 5s/step - loss: 1.6498 - accuracy: 0.3093 - val_loss: 1.6267 - val_accuracy: 0.3142\n",
            "Train Loss: 1.6168\n",
            "Train Accuracy: 0.3258\n",
            "Test Loss: 1.6243\n",
            "Test Accuracy: 0.3200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y_val = label_encoder.fit_transform(y_val)\n",
        "one_hot_y_val = to_categorical(encoded_y_val)\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(X_val, one_hot_y_val, verbose=0)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "jgZHrYASSGCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219b02e2-5fb1-4046-ecdf-c947f238e0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.6267\n",
            "Validation Accuracy: 0.3142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Retrieve training history\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Evaluate model on test data and get accuracy\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Plot accuracy curves\n",
        "plt.plot(epochs, train_accuracy, 'bo-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'ro-', label='Validation Accuracy')\n",
        "plt.axhline(y=test_accuracy, color='g', linestyle='--', label='Test Accuracy')\n",
        "plt.title('Training, Validation, and Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YpwzGh-tSHzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOSto6rcpqQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99c7f1d-b479-488a-9a0b-6a5d5c808e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 498s 791ms/step - loss: 0.4859 - accuracy: 0.7638 - val_loss: 0.3611 - val_accuracy: 0.8647\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 492s 788ms/step - loss: 0.2584 - accuracy: 0.9121 - val_loss: 0.3194 - val_accuracy: 0.8718\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 493s 788ms/step - loss: 0.1545 - accuracy: 0.9546 - val_loss: 0.3335 - val_accuracy: 0.8654\n",
            "Model trained Average Pooling Fast Text n=4...\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# NOTEEE Fast Text\n",
        "earlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=1)\n",
        "\n",
        "# Train the model\n",
        "modelHistory = combined_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, callbacks=[earlyStopping])\n",
        "\n",
        "print(\"Model trained Average Pooling Fast Text n=4...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs and batch size\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "# Train the model\n",
        "history = combined_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_cv, y_cv))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBQJ307B0BbM",
        "outputId": "028be08c-8125-4964-fbca-7f9925793c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 375s 1s/step - loss: 0.0825 - accuracy: 0.9843 - val_loss: 0.3196 - val_accuracy: 0.8840\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 371s 1s/step - loss: 0.0586 - accuracy: 0.9904 - val_loss: 0.3303 - val_accuracy: 0.8842\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 376s 1s/step - loss: 0.0435 - accuracy: 0.9937 - val_loss: 0.3503 - val_accuracy: 0.8820\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 373s 1s/step - loss: 0.0342 - accuracy: 0.9959 - val_loss: 0.3696 - val_accuracy: 0.8786\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 359s 1s/step - loss: 0.0284 - accuracy: 0.9968 - val_loss: 0.3933 - val_accuracy: 0.8744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "cnn_scores = combined_model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Testing loss:', cnn_scores[0])\n",
        "print('Testing accuracy:', cnn_scores[1])\n",
        "print('Training loss:', history.history['loss'][-1])\n",
        "print('Training accuracy:', history.history['accuracy'][-1])\n",
        "print('Validation loss:', history.history['val_loss'][-1])\n",
        "print('Validation accuracy:', history.history['val_accuracy'][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0OzzVX40CUe",
        "outputId": "40f73c49-9145-4699-90f8-8de2724f47c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 131s 167ms/step - loss: 0.4238 - accuracy: 0.8678\n",
            "Testing loss: 0.42381519079208374\n",
            "Testing accuracy: 0.8677999973297119\n",
            "Training loss: 0.028441686183214188\n",
            "Training accuracy: 0.9968000054359436\n",
            "Validation loss: 0.393278568983078\n",
            "Validation accuracy: 0.8744000196456909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lKD0gAZpqTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7312a128-0a9e-487b-e7bc-7e8410fe7fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 600)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 600, 32)      320000      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 600, 32)      3104        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 300, 32)      0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 300, 32)      4128        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 150, 32)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 150, 32)      5152        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 75, 32)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 2400)         0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 10)           24010       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 100)          53200       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 19200)        0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            11          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            101         ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            19201       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3)            0           ['dense_2[0][0]',                \n",
            "                                                                  'dense_3[0][0]',                \n",
            "                                                                  'dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            4           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 428,911\n",
            "Trainable params: 428,911\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Fast text n3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, Flatten, LSTM, concatenate\n",
        "import fasttext\n",
        "\n",
        "# Load the pre-trained FastText model\n",
        "fasttext_model = fasttext.load_model('modelngram4.bin')\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(max_review_length,))\n",
        "embedding_vector_length = 32\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, AveragePooling1D, Flatten, LSTM, concatenate\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(max_review_length,))\n",
        "embedding_vector_length = 32\n",
        "# Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=top_words, output_dim=embedding_vector_length, input_length=max_review_length)(input_layer)\n",
        "embedding_output = Dense(units=1, activation='tanh')(Flatten()(embedding_layer))\n",
        "\n",
        "# Convolutional Layer\n",
        "cnn_output = Conv1D(filters=32, kernel_size=3, padding='same', activation='tanh')(embedding_layer)\n",
        "cnn_output = MaxPooling1D(pool_size=2)(cnn_output)\n",
        "cnn_output = Conv1D(filters=32, kernel_size=4, padding='same', activation='tanh')(cnn_output)\n",
        "cnn_output = MaxPooling1D(pool_size=2)(cnn_output)\n",
        "cnn_output = Conv1D(filters=32, kernel_size=5, padding='same', activation='tanh')(cnn_output)\n",
        "cnn_output = MaxPooling1D(pool_size=2)(cnn_output)\n",
        "cnn_output = Flatten()(cnn_output)\n",
        "cnn_output = Dense(units=10, activation='relu')(cnn_output)\n",
        "cnn_output = Dense(units=1, activation='tanh')(cnn_output)\n",
        "\n",
        "# LSTM Layer\n",
        "lstm_output = LSTM(100)(embedding_layer)\n",
        "lstm_output = Dense(units=1, activation='sigmoid')(lstm_output)\n",
        "\n",
        "# Concatenate the outputs\n",
        "concatenated_output = concatenate([cnn_output, lstm_output, embedding_output])\n",
        "\n",
        "# Final Dense Layer\n",
        "output_layer = Dense(units=1, activation='sigmoid')(concatenated_output)\n",
        "\n",
        "# Create the combined model\n",
        "combined_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "combined_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "print(combined_model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# NOTEEE Fast Text\n",
        "earlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=1)\n",
        "\n",
        "# Train the model\n",
        "modelHistory = combined_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, callbacks=[earlyStopping])\n",
        "\n",
        "print(\"Model trained Max Pooling Fast Text n=4...\")"
      ],
      "metadata": {
        "id": "Q9H_fFA8rB7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs and batch size\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "# Train the model\n",
        "history = combined_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_cv, y_cv))\n",
        "\n",
        "\n",
        "# Evaluate the model on test data\n",
        "cnn_scores = combined_model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Testing loss:', cnn_scores[0])\n",
        "print('Testing accuracy:', cnn_scores[1])\n",
        "print('Training loss:', history.history['loss'][-1])\n",
        "print('Training accuracy:', history.history['accuracy'][-1])\n",
        "print('Validation loss:', history.history['val_loss'][-1])\n",
        "print('Validation accuracy:', history.history['val_accuracy'][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpNgNCWoL09m",
        "outputId": "16cb07c8-5e83-4700-a723-bd8e38015ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 201s 634ms/step - loss: 0.5199 - accuracy: 0.7423 - val_loss: 0.3585 - val_accuracy: 0.8718\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 188s 601ms/step - loss: 0.2796 - accuracy: 0.9080 - val_loss: 0.3107 - val_accuracy: 0.8812\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 188s 600ms/step - loss: 0.1860 - accuracy: 0.9455 - val_loss: 0.3067 - val_accuracy: 0.8800\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 187s 599ms/step - loss: 0.1203 - accuracy: 0.9715 - val_loss: 0.3036 - val_accuracy: 0.8874\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 189s 603ms/step - loss: 0.0773 - accuracy: 0.9880 - val_loss: 0.3161 - val_accuracy: 0.8864\n",
            "782/782 [==============================] - 62s 79ms/step - loss: 0.3322 - accuracy: 0.8762\n",
            "Testing loss: 0.3321789503097534\n",
            "Testing accuracy: 0.8762400150299072\n",
            "Training loss: 0.07727508991956711\n",
            "Training accuracy: 0.9879999756813049\n",
            "Validation loss: 0.3160579800605774\n",
            "Validation accuracy: 0.8863999843597412\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
