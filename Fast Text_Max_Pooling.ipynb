{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0t3p3gbYaQG",
        "outputId": "eb203415-34d8-4abc-e381-e7911bae004e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7_8yB6_Ycdx",
        "outputId": "6e80fef5-bf68-4c8f-f801-0b16fa2ee105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH-ykqXy0OIr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACd6SrlXmyA8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "\n",
        "#import the required library\n",
        "\n",
        "# Student will have to code here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,MaxPooling1D\n",
        "from keras.layers import LSTM, Flatten, Dropout, Conv1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtdup-novHAJ"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8WMnDjvvHsp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, concatenate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiRTf5PPd1bP",
        "outputId": "661be5b3-6c7d-4a6d-f70c-04be15608d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393216 sha256=8e4806f9fe731b38335e275a12860694ac41bcbeb1c28a6987f4dd643aba4d93\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "botqnWsxBJ1L"
      },
      "outputs": [],
      "source": [
        "fasttext.FastText.eprint = lambda x: None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLr4aJfSdvxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e6d60d-f940-42e4-ad1c-27f8529650c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.8389499822632139\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Baca dataset tweet ke dalam DataFrame\n",
        "df = pd.read_csv('Training_Testing_Data16444.csv')\n",
        "\n",
        "# Preprocessing teks jika diperlukan\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Gabungkan label dan teks ke dalam satu kolom dengan format yang diperlukan\n",
        "df['fasttext_input'] = '__label__' + df['Label'].astype(str) + ' ' + df['text']\n",
        "\n",
        "# Split data menjadi training dan test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Simpan training dan test sets ke file terpisah\n",
        "train_data['fasttext_input'].to_csv('cooking.train', index=False, header=False, sep='\\t')\n",
        "test_data['fasttext_input'].to_csv('cooking.test', index=False, header=False, sep='\\t')\n",
        "\n",
        "# Train model FastText\n",
        "model = fasttext.train_supervised(input=\"cooking.train\", lr=0.5, epoch=50, wordNgrams=3, bucket=200000, dim=50, loss='hs')\n",
        "\n",
        "# Simpan model yang dilatih\n",
        "model.save_model(\"tweet_model.bin\")\n",
        "\n",
        "# Load model yang dilatih\n",
        "model = fasttext.load_model('tweet_model.bin')\n",
        "\n",
        "# Evaluasi model pada data test\n",
        "result = model.test('cooking.test')\n",
        "\n",
        "# Ekstrak metrik evaluasi dari tuple hasil\n",
        "precision = result[1]\n",
        "recall = result[2]\n",
        "num_examples = result[0]\n",
        "\n",
        "# Cetak hasil evaluasi\n",
        "print('Akurasi:', precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MsvePWtF4gJ",
        "outputId": "c740055b-e1ad-4e72-9e8b-0c2d9867b9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8389499822632139\n"
          ]
        }
      ],
      "source": [
        "# Load the test dataset\n",
        "with open('cooking.test', 'r', encoding='utf-8') as file:\n",
        "    test_dataset = file.readlines()\n",
        "# Calculate accuracy manually\n",
        "total_examples = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "for line in test_dataset:\n",
        "    label, text = line.strip().split(' ', 1)\n",
        "    prediction = model.predict(text)[0][0]\n",
        "    if prediction == label:\n",
        "        correct_predictions += 1\n",
        "    total_examples += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkW0z2qtZ5wr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "\n",
        "#import the required library\n",
        "import numpy as np\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,MaxPooling1D\n",
        "from keras.layers import LSTM, Flatten, Dropout, Conv1D, Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, LSTM, concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import regularizers\n",
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGVPnmMBaA6u"
      },
      "outputs": [],
      "source": [
        "# Assuming 'text' is the column containing the text data in your DataFrame\n",
        "texts = df['text'].tolist()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Limit the tokenizer vocabulary to the top n words\n",
        "top_n = 10000  # Set the desired number of top words\n",
        "tokenizer.num_words = top_n\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0mRk5_SZ0Of",
        "outputId": "db891521-4413-41e6-b6f6-b719323aae23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 250\n"
          ]
        }
      ],
      "source": [
        "# Hitung panjang maksimum dari sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "print(\"Maximum sequence length:\", max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wlov3KkZ0Rf",
        "outputId": "f22e7237-75d8-4dbb-c870-ae829af1fe2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10001\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'label' is the column containing the corresponding labels in your DataFrame\n",
        "labels = df['Label'].tolist()\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = max_length  # Set the maximum sequence length based on the shape of X_train\n",
        "# Set the desired maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Split the training data further into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the vocabulary size\n",
        "vocab_size = min(top_n, len(tokenizer.word_index)) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX-yBR6VaFao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a function to encode labels and handle unseen labels\n",
        "def encode_labels(labels, classes):\n",
        "    encoded_labels = np.zeros((len(labels), len(classes)))\n",
        "    for i, label in enumerate(labels):\n",
        "        if label in classes:\n",
        "            encoded_labels[i, classes.index(label)] = 1\n",
        "        else:\n",
        "            encoded_labels[i, 0] = 1  # Assign unseen labels to the first class\n",
        "    return encoded_labels\n",
        "\n",
        "# Define the classes based on the unique labels in the original label list\n",
        "classes = np.unique(labels).tolist()\n",
        "\n",
        "# Encode labels for training, validation, and test sets\n",
        "y_train = encode_labels(y_train, classes)\n",
        "y_val = encode_labels(y_val, classes)\n",
        "y_test = encode_labels(y_test, classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQU9gmgXaIfP",
        "outputId": "0ac907c7-ab44-426b-b9e1-53ae39360f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (59198, 250)\n",
            "Shape of Test data: (24666, 250)\n",
            "Shape of CV data: (14800, 250)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)\n",
        "print(\"Shape of CV data:\", X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtUzqejmpqOG",
        "outputId": "811979b5-2aaa-42be-a814-94a8582b001e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 250)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 250, 50)      13046150    ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 248, 128)     19328       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 247, 128)     25728       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 246, 128)     32128       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 124, 128)    0           ['conv1d_4[0][0]']               \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " average_pooling1d_1 (AveragePo  (None, 123, 128)    0           ['conv1d_5[0][0]']               \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " average_pooling1d_2 (AveragePo  (None, 123, 128)    0           ['conv1d_6[0][0]']               \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['average_pooling1d[0][0]']      \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['average_pooling1d_1[0][0]']    \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 128)         0           ['average_pooling1d_2[0][0]']    \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 384)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 , 'global_average_pooling1d_1[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 12500)        0           ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 384)          0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 12500)        0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           6160        ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 64)           29440       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          1600128     ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 16)           0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 64)           0           ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 208)          0           ['dropout_7[0][0]',              \n",
            "                                                                  'dropout_8[0][0]',              \n",
            "                                                                  'dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 208)          0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 6)            1254        ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14,760,316\n",
            "Trainable params: 1,714,166\n",
            "Non-trainable params: 13,046,150\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, AveragePooling1D, GlobalAveragePooling1D, Dropout, LSTM, concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained FastText model\n",
        "fasttext_model = fasttext.load_model('tweet_model.bin')\n",
        "\n",
        "# Get the word vectors\n",
        "word_vectors = fasttext_model.get_input_matrix()\n",
        "\n",
        "# Create the embedding matrix\n",
        "vocab_size = word_vectors.shape[0]\n",
        "embedding_vector_length = word_vectors.shape[1]\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_vector_length))\n",
        "word_index = {word: idx for idx, word in enumerate(fasttext_model.words)}\n",
        "for word, index in word_index.items():\n",
        "    embedding_matrix[index] = word_vectors[index]\n",
        "\n",
        "# Define the model architecture\n",
        "max_sequence_length = 250\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False)(input_layer)\n",
        "\n",
        "# CNN Layers\n",
        "cnn_layer1 = Conv1D(128, kernel_size=3, activation='relu')(embedding_layer)\n",
        "cnn_layer1 = AveragePooling1D(pool_size=2)(cnn_layer1)\n",
        "cnn_layer2 = Conv1D(128, kernel_size=4, activation='relu')(embedding_layer)\n",
        "cnn_layer2 = AveragePooling1D(pool_size=2)(cnn_layer2)\n",
        "cnn_layer3 = Conv1D(128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "cnn_layer3 = AveragePooling1D(pool_size=2)(cnn_layer3)\n",
        "cnn_layer1 = GlobalAveragePooling1D()(cnn_layer1)\n",
        "cnn_layer2 = GlobalAveragePooling1D()(cnn_layer2)\n",
        "cnn_layer3 = GlobalAveragePooling1D()(cnn_layer3)\n",
        "cnn_layer = concatenate([cnn_layer1, cnn_layer2, cnn_layer3])\n",
        "cnn_layer = Dropout(0.5)(cnn_layer)\n",
        "cnn_layer = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01))(cnn_layer)\n",
        "cnn_layer = Dropout(0.5)(cnn_layer)\n",
        "\n",
        "# LSTM Layer\n",
        "lstm_layer = LSTM(64)(embedding_layer)\n",
        "lstm_layer = Dropout(0.5)(lstm_layer)\n",
        "\n",
        "# Embedding Layer\n",
        "embedding_only_layer = Flatten()(embedding_layer)\n",
        "embedding_only_layer = Dropout(0.5)(embedding_only_layer)\n",
        "embedding_only_layer = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding_only_layer)\n",
        "embedding_only_layer = Dropout(0.5)(embedding_only_layer)\n",
        "\n",
        "# Concatenate all layers\n",
        "merged_layer = concatenate([cnn_layer, lstm_layer, embedding_only_layer])\n",
        "merged_layer = Dropout(0.5)(merged_layer)\n",
        "output_layer = Dense(6, activation='softmax')(merged_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0OzQOApgQt1",
        "outputId": "f176e463-5257-4ea2-d041-241c2a4f8187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (59198, 250)\n",
            "Shape of validation data: (14800, 250)\n",
            "Shape of test data: (24666, 250)\n",
            "Shape of train labels: 59198\n",
            "Shape of validation labels: 14800\n",
            "Shape of test labels: 24666\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Split the training data further into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the data splits\n",
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of validation data:\", X_val.shape)\n",
        "print(\"Shape of test data:\", X_test.shape)\n",
        "print(\"Shape of train labels:\", len(y_train))\n",
        "print(\"Shape of validation labels:\", len(y_val))\n",
        "print(\"Shape of test labels:\", len(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cChoBVTgUwu",
        "outputId": "f26e6494-60aa-48fc-b328-574d9cff8c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "116/116 [==============================] - 317s 3s/step - loss: 3.1579 - accuracy: 0.2364 - val_loss: 2.1988 - val_accuracy: 0.2745\n",
            "Epoch 2/5\n",
            "116/116 [==============================] - 299s 3s/step - loss: 1.9973 - accuracy: 0.2666 - val_loss: 1.8477 - val_accuracy: 0.2888\n",
            "Epoch 3/5\n",
            "116/116 [==============================] - 314s 3s/step - loss: 1.8126 - accuracy: 0.2808 - val_loss: 1.7559 - val_accuracy: 0.2971\n",
            "Epoch 4/5\n",
            "116/116 [==============================] - 313s 3s/step - loss: 1.7393 - accuracy: 0.2885 - val_loss: 1.6875 - val_accuracy: 0.3017\n",
            "Epoch 5/5\n",
            "116/116 [==============================] - 324s 3s/step - loss: 1.6929 - accuracy: 0.2934 - val_loss: 1.6586 - val_accuracy: 0.3045\n",
            "Train Loss: 1.6534\n",
            "Train Accuracy: 0.3081\n",
            "Test Loss: 1.6546\n",
            "Test Accuracy: 0.3044\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "# Encode class labels with integers\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_train_encoded = to_categorical(y_train_encoded, num_classes)\n",
        "y_val_encoded = to_categorical(y_val_encoded, num_classes)\n",
        "y_test_encoded = to_categorical(y_test_encoded, num_classes)\n",
        "\n",
        "# Train the model with early stopping\n",
        "batch_size = 512\n",
        "epochs = 5\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train_encoded, batch_size=batch_size, epochs=epochs,\n",
        "                    validation_data=(X_val, y_val_encoded), verbose=1,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on training and test data\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded, verbose=0)\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print(f\"Train Loss: {train_loss:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
