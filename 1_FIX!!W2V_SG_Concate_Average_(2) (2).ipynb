{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0t3p3gbYaQG",
        "outputId": "e6b3cda8-ee89-4836-b673-735b7e1daba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7_8yB6_Ycdx"
      },
      "outputs": [],
      "source": [
        "# pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "IH-ykqXy0OIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e270de-4744-4aa1-f88c-1b5bfff9f219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X6y_T-9Ywg2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Membaca file CSV\n",
        "df = pd.read_csv('Training_Testing_Data16444.csv')\n",
        "\n",
        "# # Membagi data menjadi data training dan data testing\n",
        "# train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "# # test_size = 0.2 berarti 20% data akan digunakan untuk testing, sementara 80% digunakan untuk training\n",
        "# # random_state digunakan untuk mengacak data\n",
        "\n",
        "# # Menampilkan informasi jumlah data\n",
        "# print('Jumlah data training:', len(train_data))\n",
        "# print('Jumlah data testing:', len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SP-ES68aVZE"
      },
      "outputs": [],
      "source": [
        "# train_data, test_data = train_test_split(df, test_size=0.25, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data"
      ],
      "metadata": {
        "id": "J42mIlKr0XW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K10RoMqLYg8b"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# #Train Data\n",
        "\n",
        "# print(\"Data shape:\",df)\n",
        "\n",
        "# Emosi1Df = df[df[\"Label\"]==\"Gembira\"].drop(\"Label\",axis=1)\n",
        "# Emosi1Df[\"Label\"] = 0\n",
        "\n",
        "# Emosi2Df = df[df[\"Label\"]==\"Sedih\"].drop(\"Label\",axis=1)\n",
        "# Emosi2Df[\"Label\"] = 1\n",
        "\n",
        "# Emosi3Df = df[df[\"Label\"]==\"Muak\"].drop(\"Label\",axis=1)\n",
        "# Emosi3Df[\"Label\"] = 2\n",
        "\n",
        "# Emosi4Df = df[df[\"Label\"]==\"Marah\"].drop(\"Label\",axis=1)\n",
        "# Emosi4Df[\"Label\"] = 3\n",
        "\n",
        "# Emosi5Df = df[df[\"Label\"]==\"Takut\"].drop(\"Label\",axis=1)\n",
        "# Emosi5Df[\"Label\"] = 4\n",
        "\n",
        "# Emosi6Df = df[df[\"Label\"]==\"Terkejut\"].drop(\"Label\",axis=1)\n",
        "# Emosi6Df[\"Label\"] = 5\n",
        "\n",
        "# df = pd.concat([Emosi1Df,Emosi2Df,Emosi3Df,Emosi4Df,Emosi5Df,Emosi6Df],ignore_index=True)\n",
        "# print(\"Final total dataset:\",df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeN_xjWtYg-7"
      },
      "outputs": [],
      "source": [
        "# train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGW5GqHVYhBV"
      },
      "outputs": [],
      "source": [
        "# # Simpan hasil ke file CSV\n",
        "# df.to_csv('output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Membaca file CSV\n",
        "# df = pd.read_csv('output (1).csv')"
      ],
      "metadata": {
        "id": "x3wIaE_myQyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI8cWS2EYhDf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Menggunakan tokenizer dari NLTK\n",
        "# df['text'] = df['text'].apply(word_tokenize)\n",
        "\n",
        "# # Menampilkan DataFrame dengan teks yang telah di-tokenisasi\n",
        "# print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP2k7EQTfuII"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Assuming 'text' is the column containing the text data in your DataFrame\n",
        "list_of_sent = [word_tokenize(str(sent)) for sent in df[\"text\"]]\n",
        "w2v_model = gensim.models.Word2Vec(list_of_sent,vector_size=200,window=3,min_count=2,sg=1,negative=10, ns_exponent=0.75, workers=4,epochs=10)#Skipgram sg=1\n",
        "w2v_words = list(w2v_model.wv.index_to_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Word2Vec model\n",
        "model_path = \"model_sg.bin\"  # Ganti dengan path tempat Anda ingin menyimpan model\n",
        "w2v_model.save(model_path)\n"
      ],
      "metadata": {
        "id": "OW9AFyW9TLAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WNbg_n4ggfx",
        "outputId": "bdfb9d7c-ce6d-47fe-8eff-883ed49ab1b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30955"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(w2v_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YJMTH-mgjcH",
        "outputId": "4dc78734-fb50-47b9-a9e7-15c55ed41cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98664/98664 [01:08<00:00, 1445.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6  \\\n",
            "0  1.242961 -0.643272  1.018399  0.715394 -0.270445 -1.324530  0.869032   \n",
            "1 -1.801012 -1.158490  0.033325 -0.190662  0.645363  0.242590  0.910903   \n",
            "2 -0.914484  1.193085 -0.941452 -0.640334  0.087640  0.942915  0.540623   \n",
            "3  0.786590  0.578556  1.001725 -0.433143 -0.025089  0.037009  0.041258   \n",
            "4 -0.120129 -1.394775  1.246648 -1.373574  0.220639 -0.348208 -0.548035   \n",
            "5  0.166675 -0.098405  0.371948 -0.162663  0.196284 -0.024860 -0.163535   \n",
            "6 -0.303381  0.746830 -0.442223 -1.201420  0.337949  1.564170 -0.172779   \n",
            "7  0.455673  0.991878 -0.079048 -0.317153 -0.249131  0.411229  1.118792   \n",
            "8  0.500198 -1.144849  0.640001 -0.373842 -0.014655 -1.023570 -1.383252   \n",
            "9 -1.732755  0.222601 -0.395225 -0.780365  0.641238  0.970716  0.098041   \n",
            "\n",
            "          7         8         9  ...       191       192       193       194  \\\n",
            "0 -0.494344 -0.244893  0.513347  ...  0.606476 -0.312372 -0.456044 -1.566594   \n",
            "1 -0.516855 -0.512269 -1.708137  ...  0.095676  0.111757  1.678191 -1.184113   \n",
            "2 -0.133474  0.363092  0.275996  ...  0.554218 -0.137689  0.263891  0.589040   \n",
            "3 -0.154055 -0.309958  0.424185  ... -0.774160  0.134477  0.856265  1.059017   \n",
            "4  1.028701  0.868848  0.020225  ... -0.856353 -0.862113  0.059024  0.605116   \n",
            "5  0.328215 -0.544278 -0.267216  ... -1.037844 -0.466956  1.459788 -0.537248   \n",
            "6  1.502710  2.384448 -2.114403  ...  1.083065  0.840379 -1.044087  0.691232   \n",
            "7  0.788124 -0.639014 -1.101611  ... -1.158678  0.226098  1.318183  1.541483   \n",
            "8  1.782793  0.366731 -1.650687  ... -0.949538  0.251956 -0.145901 -0.124867   \n",
            "9 -1.140541  0.942683  0.288990  ...  0.113152  0.662157 -0.232098 -0.793348   \n",
            "\n",
            "        195       196       197       198       199  Label  \n",
            "0  0.231796  1.192240  0.806700 -0.374922  0.174523  Sedih  \n",
            "1 -0.792409  0.409457 -0.883305 -1.010721 -0.092542  Sedih  \n",
            "2  0.469661  0.350917  0.656128 -0.688662 -0.923679  Sedih  \n",
            "3 -0.303308  0.945010 -1.286890 -0.601334  0.241123  Sedih  \n",
            "4  0.889548 -0.367474 -0.879570 -0.368672  0.526878  Sedih  \n",
            "5 -0.113611  0.232820 -0.743619  0.202587  0.078552  Sedih  \n",
            "6 -0.539995 -1.501536  1.170537 -0.146790  0.009861  Sedih  \n",
            "7 -0.080433 -0.581438 -0.932851  0.359478  0.560441  Sedih  \n",
            "8 -0.086369 -0.500504  0.051836  0.472684 -1.138131  Sedih  \n",
            "9 -0.570773 -0.375673 -0.571467 -1.182550  0.583581  Sedih  \n",
            "\n",
            "[10 rows x 201 columns]\n",
            "(98664, 201)\n"
          ]
        }
      ],
      "source": [
        "#Vectorize train text data\n",
        "listof_sent_vec=[]\n",
        "for sent in tqdm(list_of_sent):\n",
        "    sent_vec = np.zeros(200)\n",
        "    cnt_words =0;\n",
        "    for word in sent:\n",
        "        if word in w2v_words:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        sent_vec /= cnt_words\n",
        "    listof_sent_vec.append(sent_vec)\n",
        "\n",
        "Label = df[\"Label\"]\n",
        "list_col=tuple(range(200))\n",
        "Scaler = StandardScaler()\n",
        "data_vec = Scaler.fit_transform(listof_sent_vec)\n",
        "W2v_data=pd.DataFrame(data=data_vec, columns=list_col)\n",
        "W2v_data[\"Label\"] = Label\n",
        "print(W2v_data.head(10))\n",
        "print(W2v_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYAaDyDCgsm9",
        "outputId": "49e98216-928d-40e7-9e24-47cdba986337"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.24296074, -0.64327217,  1.0183992 , ...,  0.80669981,\n",
              "        -0.37492206,  0.17452345],\n",
              "       [-1.80101167, -1.15848953,  0.03332522, ..., -0.88330463,\n",
              "        -1.01072072, -0.09254181],\n",
              "       [-0.91448441,  1.19308497, -0.9414521 , ...,  0.65612795,\n",
              "        -0.68866175, -0.9236795 ],\n",
              "       ...,\n",
              "       [ 0.92531608, -1.94159433,  1.7033243 , ..., -0.46898184,\n",
              "        -0.3182432 ,  1.34929006],\n",
              "       [ 0.57416666,  0.29251189,  0.07056491, ...,  0.46071642,\n",
              "         1.00639554, -0.04385427],\n",
              "       [ 1.33838942, -0.08529013,  0.3053093 , ..., -0.99143742,\n",
              "         0.83502975, -1.43788548]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\n",
        "X = W2v_data.drop(\"Label\",axis=1).to_numpy()\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Word2Vec model\n",
        "from gensim.models import Word2Vec\n",
        "model_path = \"model_sg.bin\"  # Replace with the path where you saved the model\n",
        "w2v_model = Word2Vec.load(model_path)"
      ],
      "metadata": {
        "id": "7L0f-v9a_8NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'label' is the column containing the corresponding labels in your DataFrame\n",
        "labels = df['Label'].tolist()\n",
        "\n",
        "# Initialize label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode labels to integers\n",
        "encoded_labels = label_encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "DpqueXEtDwnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_Ua3slcgwyI",
        "outputId": "4bac5a72-3af5-44cf-e08a-77d680efbf09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Sedih', 'Sedih', 'Sedih', 'Sedih', 'Sedih'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y = W2v_data[\"Label\"].to_numpy()\n",
        "y[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNbSJS2Wnb9s",
        "outputId": "680cbeb7-e683-4f94-c178-a7fdb9eda526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade keras tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACd6SrlXmyA8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "\n",
        "#import the required library\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,MaxPooling1D\n",
        "from keras.layers import LSTM, Flatten, Dropout, Conv1D, Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'text' is the column containing the text data in your DataFrame\n",
        "texts = df['text'].tolist()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Limit the tokenizer vocabulary to the top n words\n",
        "top_n = 10000  # Set the desired number of top words\n",
        "tokenizer.num_words = top_n\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n"
      ],
      "metadata": {
        "id": "8hQQGU0taX9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung panjang maksimum dari sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "print(\"Maximum sequence length:\", max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_dTHVEhdIvr",
        "outputId": "ad7f4459-3452-4ff7-b6ea-9a62e542d09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'label' is the column containing the corresponding labels in your DataFrame\n",
        "labels = df['Label'].tolist()\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = max_length  # Set the maximum sequence length based on the shape of X_train\n",
        "# Set the desired maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Split the training data further into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the vocabulary size\n",
        "vocab_size = min(top_n, len(tokenizer.word_index)) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3XqARhze2VL",
        "outputId": "d54cda16-817f-4b0c-b880-0ba132b71cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert labels to categorical representation\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "BhrSLTusJ3AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoded_texts = tokenizer.sequences_to_texts(sequences)\n",
        "# decoded_labels = np.array(labels)\n",
        "\n",
        "# # Print the decoded texts and labels\n",
        "# print(decoded_texts)\n",
        "# print(decoded_labels)\n"
      ],
      "metadata": {
        "id": "PQiasDVcdNc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7qM93KXoqaJ",
        "outputId": "59df0449-0179-4369-8c07-61a6a29c38e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (59198, 250)\n",
            "Shape of Test data: (24666, 250)\n",
            "Shape of CV data: (14800, 250)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)\n",
        "print(\"Shape of CV data:\", X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert labels to categorical representation\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_train = label_encoder.fit_transform(y_train)\n",
        "# y_val = label_encoder.transform(y_val)\n",
        "# y_test = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "w2t8sPXGunpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAK9xPpaoqdp"
      },
      "outputs": [],
      "source": [
        "max_review_length = 250\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "X_cv = sequence.pad_sequences(X_val, maxlen=max_review_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CgUrD9Q1xwt",
        "outputId": "44634ae8-daf8-49d6-ce5d-e30a011caad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 5, 4, 4, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoded_texts = tokenizer.sequences_to_texts(sequences)\n",
        "# decoded_labels = np.array(labels)\n",
        "\n",
        "# # Print the decoded texts and labels\n",
        "# print(decoded_texts)\n",
        "# print(decoded_labels)\n"
      ],
      "metadata": {
        "id": "5kTWnTuvi_Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Conv1D, AveragePooling1D, GlobalAveragePooling1D, concatenate, Dense, Dropout, Flatten, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the Word2Vec model\n",
        "model_path = \"model_sg.bin\"  # Replace with the path where you saved the model\n",
        "w2v_model = Word2Vec.load(model_path)\n",
        "\n",
        "# Get the word vectors\n",
        "word_vectors = w2v_model.wv\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_vector_length = 200  # Adjust according to the dimension of the word vectors in your Word2Vec model\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_vector_length))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index < vocab_size and word in word_vectors:\n",
        "        embedding_matrix[index] = word_vectors[word]"
      ],
      "metadata": {
        "id": "8n-BcyOupWHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False)(input_layer)\n",
        "\n",
        "# CNN Layers\n",
        "cnn_layer1 = Conv1D(128, kernel_size=3, activation='relu')(embedding_layer)\n",
        "cnn_layer1 = AveragePooling1D(pool_size=2)(cnn_layer1)\n",
        "cnn_layer2 = Conv1D(128, kernel_size=4, activation='relu')(embedding_layer)\n",
        "cnn_layer2 = AveragePooling1D(pool_size=2)(cnn_layer2)\n",
        "cnn_layer3 = Conv1D(128, kernel_size=5, activation='relu')(embedding_layer)\n",
        "cnn_layer3 = AveragePooling1D(pool_size=2)(cnn_layer3)\n",
        "cnn_layer1 = GlobalAveragePooling1D()(cnn_layer1)  # Adjusted\n",
        "cnn_layer2 = GlobalAveragePooling1D()(cnn_layer2)  # Adjusted\n",
        "cnn_layer3 = GlobalAveragePooling1D()(cnn_layer3)  # Adjusted\n",
        "cnn_layer = concatenate([cnn_layer1, cnn_layer2, cnn_layer3])\n",
        "cnn_layer = Dropout(0.5)(cnn_layer)\n",
        "cnn_layer = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01))(cnn_layer)\n",
        "cnn_layer = Dropout(0.5)(cnn_layer)\n",
        "\n",
        "# LSTM Layer\n",
        "lstm_layer = Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False)(input_layer)\n",
        "lstm_layer = LSTM(64)(lstm_layer)  # Modified to a single LSTM layer\n",
        "lstm_layer = Dropout(0.5)(lstm_layer)\n",
        "\n",
        "# Embedding Layer\n",
        "embedding_only_layer = Flatten()(embedding_layer)\n",
        "embedding_only_layer = Dropout(0.5)(embedding_only_layer)\n",
        "embedding_only_layer = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding_only_layer)\n",
        "embedding_only_layer = Dropout(0.5)(embedding_only_layer)\n",
        "\n",
        "# Concatenate all layers\n",
        "merged_layer = concatenate([cnn_layer, lstm_layer, embedding_only_layer])\n",
        "merged_layer = Dropout(0.5)(merged_layer)\n",
        "output_layer = Dense(6, activation='softmax')(merged_layer)\n",
        "\n"
      ],
      "metadata": {
        "id": "1mQSQ7y_mY07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with categorical crossentropy loss\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esSobgnGmfPe",
        "outputId": "298b1264-ddf5-4cfe-9c8c-d602c3906b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 250)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 250, 200)     2000200     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 248, 128)     76928       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 247, 128)     102528      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 246, 128)     128128      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 124, 128)    0           ['conv1d[0][0]']                 \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " average_pooling1d_1 (AveragePo  (None, 123, 128)    0           ['conv1d_1[0][0]']               \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " average_pooling1d_2 (AveragePo  (None, 123, 128)    0           ['conv1d_2[0][0]']               \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['average_pooling1d[0][0]']      \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['average_pooling1d_1[0][0]']    \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 128)         0           ['average_pooling1d_2[0][0]']    \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 384)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 , 'global_average_pooling1d_1[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 50000)        0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 384)          0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 250, 200)     2000200     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 50000)        0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           6160        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 64)           67840       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          6400128     ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 16)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64)           0           ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 208)          0           ['dropout_1[0][0]',              \n",
            "                                                                  'dropout_2[0][0]',              \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 208)          0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 6)            1254        ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,783,366\n",
            "Trainable params: 6,782,966\n",
            "Non-trainable params: 4,000,400\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Split the training data further into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the data splits\n",
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of validation data:\", X_val.shape)\n",
        "print(\"Shape of test data:\", X_test.shape)\n",
        "print(\"Shape of train labels:\", len(y_train))\n",
        "print(\"Shape of validation labels:\", len(y_val))\n",
        "print(\"Shape of test labels:\", len(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0prByyS1GEU7",
        "outputId": "e36dcae2-550c-4d7c-82ad-560a7271dcab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (59198, 250)\n",
            "Shape of validation data: (14800, 250)\n",
            "Shape of test data: (24666, 250)\n",
            "Shape of train labels: 59198\n",
            "Shape of validation labels: 14800\n",
            "Shape of test labels: 24666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "batch_size = 64  # Increase the batch size if memory allows\n",
        "epochs = 100  # Increase the number of epochs\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "                    validation_data=(X_val, y_val), verbose=1,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on training and test data\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Train Loss: {train_loss:.4f}\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRqJM0GQm11f",
        "outputId": "70318aa2-93a4-4710-aaf2-2e25c8f4b5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "925/925 [==============================] - 509s 548ms/step - loss: 1.2795 - accuracy: 0.5891 - val_loss: 0.8074 - val_accuracy: 0.7407\n",
            "Epoch 2/100\n",
            "925/925 [==============================] - 503s 544ms/step - loss: 0.7924 - accuracy: 0.7358 - val_loss: 0.6149 - val_accuracy: 0.7828\n",
            "Epoch 3/100\n",
            "925/925 [==============================] - 542s 586ms/step - loss: 0.6684 - accuracy: 0.7695 - val_loss: 0.5559 - val_accuracy: 0.7989\n",
            "Epoch 4/100\n",
            "925/925 [==============================] - 503s 544ms/step - loss: 0.6130 - accuracy: 0.7887 - val_loss: 0.5219 - val_accuracy: 0.8146\n",
            "Epoch 5/100\n",
            "925/925 [==============================] - 502s 543ms/step - loss: 0.5787 - accuracy: 0.7981 - val_loss: 0.4956 - val_accuracy: 0.8214\n",
            "Epoch 6/100\n",
            "925/925 [==============================] - 503s 543ms/step - loss: 0.5440 - accuracy: 0.8105 - val_loss: 0.4781 - val_accuracy: 0.8242\n",
            "Epoch 7/100\n",
            "925/925 [==============================] - 502s 543ms/step - loss: 0.5246 - accuracy: 0.8172 - val_loss: 0.4579 - val_accuracy: 0.8322\n",
            "Epoch 8/100\n",
            "925/925 [==============================] - 503s 544ms/step - loss: 0.5050 - accuracy: 0.8240 - val_loss: 0.4522 - val_accuracy: 0.8354\n",
            "Epoch 9/100\n",
            "925/925 [==============================] - 502s 542ms/step - loss: 0.4925 - accuracy: 0.8291 - val_loss: 0.4474 - val_accuracy: 0.8367\n",
            "Epoch 10/100\n",
            "925/925 [==============================] - 502s 542ms/step - loss: 0.4746 - accuracy: 0.8339 - val_loss: 0.4301 - val_accuracy: 0.8420\n",
            "Epoch 11/100\n",
            "925/925 [==============================] - 503s 544ms/step - loss: 0.4657 - accuracy: 0.8368 - val_loss: 0.4292 - val_accuracy: 0.8427\n",
            "Epoch 12/100\n",
            "925/925 [==============================] - 502s 542ms/step - loss: 0.4510 - accuracy: 0.8406 - val_loss: 0.4351 - val_accuracy: 0.8430\n",
            "Epoch 13/100\n",
            "925/925 [==============================] - 505s 546ms/step - loss: 0.4413 - accuracy: 0.8440 - val_loss: 0.4188 - val_accuracy: 0.8464\n",
            "Epoch 14/100\n",
            "925/925 [==============================] - 503s 544ms/step - loss: 0.4310 - accuracy: 0.8480 - val_loss: 0.4149 - val_accuracy: 0.8464\n",
            "Epoch 15/100\n",
            "925/925 [==============================] - 503s 543ms/step - loss: 0.4213 - accuracy: 0.8511 - val_loss: 0.4208 - val_accuracy: 0.8492\n",
            "Epoch 16/100\n",
            "925/925 [==============================] - 505s 546ms/step - loss: 0.4151 - accuracy: 0.8539 - val_loss: 0.4149 - val_accuracy: 0.8514\n",
            "Epoch 17/100\n",
            "925/925 [==============================] - 506s 546ms/step - loss: 0.4068 - accuracy: 0.8558 - val_loss: 0.4084 - val_accuracy: 0.8519\n",
            "Epoch 18/100\n",
            "925/925 [==============================] - 503s 544ms/step - loss: 0.4013 - accuracy: 0.8595 - val_loss: 0.4094 - val_accuracy: 0.8534\n",
            "Epoch 19/100\n",
            "925/925 [==============================] - 503s 543ms/step - loss: 0.3930 - accuracy: 0.8615 - val_loss: 0.4032 - val_accuracy: 0.8539\n",
            "Epoch 20/100\n",
            "925/925 [==============================] - 546s 591ms/step - loss: 0.3869 - accuracy: 0.8622 - val_loss: 0.4013 - val_accuracy: 0.8558\n",
            "Epoch 21/100\n",
            "925/925 [==============================] - 506s 547ms/step - loss: 0.3832 - accuracy: 0.8639 - val_loss: 0.4224 - val_accuracy: 0.8524\n",
            "Epoch 22/100\n",
            "925/925 [==============================] - 504s 545ms/step - loss: 0.3794 - accuracy: 0.8649 - val_loss: 0.4093 - val_accuracy: 0.8514\n",
            "Epoch 23/100\n",
            "925/925 [==============================] - 504s 545ms/step - loss: 0.3747 - accuracy: 0.8676 - val_loss: 0.5323 - val_accuracy: 0.8132\n",
            "Train Loss: 0.4309\n",
            "Train Accuracy: 0.8425\n",
            "Test Loss: 0.5533\n",
            "Test Accuracy: 0.8070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a74JcHKWGxAR",
        "outputId": "4b8093f6-41b2-48ea-dff9-cf46fad4fd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.5323\n",
            "Validation Accuracy: 0.8132\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}